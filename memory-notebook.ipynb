{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing some input keys: {'movie'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m\n\u001b[1;32m     32\u001b[0m example_prompt \u001b[38;5;241m=\u001b[39m  ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages([\n\u001b[1;32m     33\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124më‹¹ì‹ ì€ ì˜í™” ìš”ì•½ GPTì…ë‹ˆë‹¤. ì œëª©ì„ ì œì‹œí•˜ë©´, ì„¸ê°œì˜ ì´ëª¨ì§€ë¡œ ìš”ì•½í•´ì¤ë‹ˆë‹¤.  \u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     34\u001b[0m     few_shot_prompt,\n\u001b[1;32m     35\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{movie}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m ])\n\u001b[1;32m     38\u001b[0m chat_llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39mchat, prompt\u001b[38;5;241m=\u001b[39mexample_prompt, memory\u001b[38;5;241m=\u001b[39mmemory)\n\u001b[0;32m---> 39\u001b[0m \u001b[43mchat_llm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhuman_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43më§ˆì§€ë§‰ìœ¼ë¡œ ì§ˆë¬¸í•œ ì˜í™”ì—ëŒ€í•œ ì •ë³´ë¥¼ ì•Œë ¤ì¤˜\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/study/gpt/fullstack-gpt/env/lib/python3.10/site-packages/langchain/chains/llm.py:298\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    284\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/study/gpt/fullstack-gpt/env/lib/python3.10/site-packages/langchain/chains/base.py:286\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    252\u001b[0m     inputs: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    259\u001b[0m     include_run_info: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    260\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    261\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m            `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprep_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m     callback_manager \u001b[38;5;241m=\u001b[39m CallbackManager\u001b[38;5;241m.\u001b[39mconfigure(\n\u001b[1;32m    288\u001b[0m         callbacks,\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata,\n\u001b[1;32m    295\u001b[0m     )\n\u001b[1;32m    296\u001b[0m     new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/study/gpt/fullstack-gpt/env/lib/python3.10/site-packages/langchain/chains/base.py:443\u001b[0m, in \u001b[0;36mChain.prep_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    441\u001b[0m     external_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mload_memory_variables(inputs)\n\u001b[1;32m    442\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexternal_context)\n\u001b[0;32m--> 443\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "File \u001b[0;32m~/study/gpt/fullstack-gpt/env/lib/python3.10/site-packages/langchain/chains/base.py:195\u001b[0m, in \u001b[0;36mChain._validate_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    193\u001b[0m missing_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_keys)\u001b[38;5;241m.\u001b[39mdifference(inputs)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_keys:\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing some input keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Missing some input keys: {'movie'}"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import  ChatOpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"movie\":\"íƒ‘ê±´\", \"summary\":\"ğŸ›©ï¸ğŸ‘¨â€âœˆï¸ğŸ”¥\" },\n",
    "    {\"movie\": \"ëŒ€ë¶€\", \"summary\" :\"ğŸ‘¨â€ğŸ‘¨â€ğŸ‘¦ğŸ”«ğŸ\"}]\n",
    "\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{movie}\"),\n",
    "    (\"ai\", \"{summary}\"),\n",
    "])\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini-2024-07-18\",\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")\n",
    "\n",
    "memory = ConversationBufferWindowMemory(LLM=chat, window_size=10, return_messages=True)\n",
    "\n",
    "\n",
    "example_prompt =  ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ë‹¹ì‹ ì€ ì˜í™” ìš”ì•½ GPTì…ë‹ˆë‹¤. ì œëª©ì„ ì œì‹œí•˜ë©´, ì„¸ê°œì˜ ì´ëª¨ì§€ë¡œ ìš”ì•½í•´ì¤ë‹ˆë‹¤.  \"),\n",
    "    few_shot_prompt,\n",
    "    (\"human\", \"{movie}\")\n",
    "])\n",
    "\n",
    "chat_llm_chain = LLMChain(llm=chat, prompt=example_prompt, memory=memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
