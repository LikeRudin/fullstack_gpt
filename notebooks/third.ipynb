{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: Aaronson 은 유죄인가요?\n",
      "\"\"''No. I believe it. I KNOW that you will fail. There is something in the universe--I don't know, some spirit, some principle--that you will never overcome.''\"\"\"\"죄에 대한 정보가 없기 때문에 Aaronson이 유죄인지 여부에 대해 알 수 없습니다. 추가적인 정보가 필요합니다.답변: content='죄에 대한 정보가 없기 때문에 Aaronson이 유죄인지 여부에 대해 알 수 없습니다. 추가적인 정보가 필요합니다.'\n",
      "\n",
      "질문: 그가 테이블에 어떤 메시지를 썼나요?\n",
      "이 텍스트에는 그가 테이블에 어떤 메시지를 썼다는 내용이 포함되어 있지 않습니다. 따라서 관련된 텍스트는 없습니다.그는 테이블에 다음과 같은 메시지를 썼습니다: \n",
      "\n",
      "\"FREEDOM IS SLAVERY\" \n",
      "\n",
      "그리고 그 아래에 썼습니다: \n",
      "\n",
      "\"TWO AND TWO MAKE FIVE\" \n",
      "\n",
      "마지막으로 그는 다음을 썼습니다: \n",
      "\n",
      "\"GOD IS POWER\"''Tell me,' he said, 'how soon will they shoot me?''''The worst thing in the world,' said O'Brien, 'varies from individual to individual. It may be burial alive, or death by fire, or by drowning, or by impalement, or fifty other deaths. There are cases where it is some quite trivial thing, not even fatal.' ''죄에 대한 정보가 없기 때문에 Aaronson이 유죄인지 여부에 대해 알 수 없습니다. 추가적인 정보가 필요합니다. 테이블에 어떤 메시지를 썼는지에 대한 정보는 제공되지 않았습니다.답변: content='죄에 대한 정보가 없기 때문에 Aaronson이 유죄인지 여부에 대해 알 수 없습니다. 추가적인 정보가 필요합니다. 테이블에 어떤 메시지를 썼는지에 대한 정보는 제공되지 않았습니다.'\n",
      "\n",
      "질문: Julia 는 누구인가요?\n",
      "'Julia! Julia! Julia, my love! Julia!'''\"\"이 텍스트에는 Julia에 대한 직접적인 설명은 없지만, 그녀는 주인공과 함께 있었던 인물로 보입니다. \"그는 Julia와 함께 있었던 순간들\"이라는 언급이 있으며, 그들의 관계가 중요하다는 암시가 있습니다. 그러나 Julia에 대한 구체적인 정보는 제공되지 않습니다.죄송하지만, Julia에 대한 정보는 제공되지 않았습니다. 추가적인 정보가 필요합니다.답변: content='죄송하지만, Julia에 대한 정보는 제공되지 않았습니다. 추가적인 정보가 필요합니다.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/document.txt\")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "documents = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "vectorstore = FAISS.from_documents(documents, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    llm=llm,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "\n",
    "basic_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        You are a helpful assistant here to assist the user.\n",
    "        The user may ask questions and provide previous conversation history with you. \n",
    "        Your task is to answer the current question by leveraging both the provided context and the conversation history. \n",
    "        If you don’t know the answer, simply say you don’t know—please avoid making up information. \n",
    "        Use only the information from the context and chat history to formulate your response.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\"human\", \"Conversation history:\\n{chat_history}\\n\\nCurrent question: {question}\")\n",
    "])\n",
    "\n",
    "circuit_search_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following excerpt from a longer document to determine if any of the text is relevant for answering the question. \n",
    "            Return any relevant text verbatim. If there is no relevant text, return an empty string.\n",
    "            -------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "repeat_chain = circuit_search_prompt | llm\n",
    "\n",
    "def process_document(document, question):\n",
    "    time.sleep(30)\n",
    "    result = repeat_chain.invoke({\"context\": document.page_content, \"question\": question})\n",
    "    return result.content\n",
    "\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "\n",
    "    results = []\n",
    "    for doc in documents:\n",
    "        results.append(process_document(doc, question))\n",
    "    return \"\\n\\n\".join(results)\n",
    "\n",
    "map_chain = {\n",
    "    \"documents\": retriever,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnableLambda(map_docs)\n",
    "\n",
    "final_chain = {\n",
    "    \"context\": map_chain,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"chat_history\": lambda _: memory.load_memory_variables({})[\"chat_history\"]\n",
    "} | basic_prompt | llm\n",
    "\n",
    "\n",
    "questions = [\"Aaronson 은 유죄인가요?\", \"그가 테이블에 어떤 메시지를 썼나요?\", \"Julia 는 누구인가요?\"]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"질문: {question}\")\n",
    "    response = final_chain.invoke(question)\n",
    "    print(f\"답변: {response}\\n\")\n",
    "    memory.save_context({\"question\": question}, {\"response\": str(response)})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
